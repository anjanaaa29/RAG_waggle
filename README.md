# ğŸ¤– MyWaggle FAQ RAG Chatbot

A simple **Retrieval-Augmented Generation (RAG) chatbot** built to answer questions from the **MyWaggle FAQ website**.  
It retrieves relevant FAQ content using **FAISS vector search** and generates accurate, context-aware answers using **Groq LLM**.

---

## ğŸ”— Source Website (FAQ Data)

ğŸ‘‰ https://support.mywaggle.com/

---

## ğŸ”„ Workflow (RAG Pipeline)

User Question
â†“
Convert question to embedding
â†“
FAISS vector search (Top-K FAQ chunks)
â†“
Relevant FAQ context
â†“
Groq LLM (LLaMA 3.1)
â†“
Final Answer shown to user

---

## ğŸ§  Architecture Overview

- **Retriever**: FAISS (semantic similarity search)
- **Embeddings**: SentenceTransformers (`all-MiniLM-L6-v2`)
- **Generator**: Groq LLM
- **Frontend**: Streamlit

---

## ğŸ§© File Descriptions (One-Liners)

- **scrape_faq.py**  
  Scrapes FAQ questions and answers from the MyWaggle support website.

- **preprocess.py**  
  Cleans the scraped text and splits it into smaller overlapping chunks.

- **build_embeddings.py**  
  Converts text chunks into embeddings and stores them in a FAISS index.

- **rag.py**  
  Handles retrieval of relevant chunks and generates answers using Groq LLM.

- **app.py**  
  Streamlit UI for interacting with the chatbot (question input, answer display, clear button).


---

## ğŸ“¦ Install Dependencies

### Download & install requirements

```bash
pip install -r requirements.txt
```
---
â–¶ï¸ Run the Application Locally
streamlit run app.py

ğŸŒ Streamlit Cloud Deployment

Once deployed, the app will be available at:


ğŸ‘‰ ](https://ragwaggle-chatbot12345.streamlit.app/)

